{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The main purpose is to build a deep learning model to classify SMS messages as spam or ham .It preprocesses text, vectorizes it using TF-IDF, trains a PyTorch ANN model, and evaluates its performance. The best precision achieved was 0.9432, and the model  predicts whether a new message is SPAM or HAM."
      ],
      "metadata": {
        "id": "PBYLTK-l91XR"
      },
      "id": "PBYLTK-l91XR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88572e97",
      "metadata": {
        "id": "88572e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f04a34-b959-4c84-c1ab-35d82abf7627",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9849    0.9768    0.9808      1206\n",
            "         1.0     0.8579    0.9037    0.8802       187\n",
            "\n",
            "    accuracy                         0.9670      1393\n",
            "   macro avg     0.9214    0.9403    0.9305      1393\n",
            "weighted avg     0.9679    0.9670    0.9673      1393\n",
            "\n",
            "Epoch 1: Precision=0.8579 <-- Best Precision!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9881    0.9677    0.9778      1206\n",
            "         1.0     0.8160    0.9251    0.8672       187\n",
            "\n",
            "    accuracy                         0.9620      1393\n",
            "   macro avg     0.9021    0.9464    0.9225      1393\n",
            "weighted avg     0.9650    0.9620    0.9629      1393\n",
            "\n",
            "Epoch 2: Precision=0.8160\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9889    0.9635    0.9761      1206\n",
            "         1.0     0.7982    0.9305    0.8593       187\n",
            "\n",
            "    accuracy                         0.9591      1393\n",
            "   macro avg     0.8936    0.9470    0.9177      1393\n",
            "weighted avg     0.9633    0.9591    0.9604      1393\n",
            "\n",
            "Epoch 3: Precision=0.7982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9883    0.9776    0.9829      1206\n",
            "         1.0     0.8650    0.9251    0.8941       187\n",
            "\n",
            "    accuracy                         0.9706      1393\n",
            "   macro avg     0.9266    0.9514    0.9385      1393\n",
            "weighted avg     0.9717    0.9706    0.9710      1393\n",
            "\n",
            "Epoch 4: Precision=0.8650 <-- Best Precision!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9867    0.9859    0.9863      1206\n",
            "         1.0     0.9096    0.9144    0.9120       187\n",
            "\n",
            "    accuracy                         0.9763      1393\n",
            "   macro avg     0.9481    0.9502    0.9492      1393\n",
            "weighted avg     0.9764    0.9763    0.9763      1393\n",
            "\n",
            "Epoch 5: Precision=0.9096 <-- Best Precision!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9835    0.9892    0.9864      1206\n",
            "         1.0     0.9278    0.8930    0.9101       187\n",
            "\n",
            "    accuracy                         0.9763      1393\n",
            "   macro avg     0.9556    0.9411    0.9482      1393\n",
            "weighted avg     0.9760    0.9763    0.9761      1393\n",
            "\n",
            "Epoch 6: Precision=0.9278 <-- Best Precision!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9827    0.9917    0.9872      1206\n",
            "         1.0     0.9432    0.8877    0.9146       187\n",
            "\n",
            "    accuracy                         0.9777      1393\n",
            "   macro avg     0.9630    0.9397    0.9509      1393\n",
            "weighted avg     0.9774    0.9777    0.9775      1393\n",
            "\n",
            "Epoch 7: Precision=0.9432 <-- Best Precision!\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9827    0.9909    0.9868      1206\n",
            "         1.0     0.9379    0.8877    0.9121       187\n",
            "\n",
            "    accuracy                         0.9770      1393\n",
            "   macro avg     0.9603    0.9393    0.9494      1393\n",
            "weighted avg     0.9767    0.9770    0.9768      1393\n",
            "\n",
            "Epoch 8: Precision=0.9379\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9827    0.9900    0.9864      1206\n",
            "         1.0     0.9326    0.8877    0.9096       187\n",
            "\n",
            "    accuracy                         0.9763      1393\n",
            "   macro avg     0.9577    0.9389    0.9480      1393\n",
            "weighted avg     0.9760    0.9763    0.9761      1393\n",
            "\n",
            "Epoch 9: Precision=0.9326\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9819    0.9909    0.9864      1206\n",
            "         1.0     0.9375    0.8824    0.9091       187\n",
            "\n",
            "    accuracy                         0.9763      1393\n",
            "   macro avg     0.9597    0.9366    0.9477      1393\n",
            "weighted avg     0.9760    0.9763    0.9760      1393\n",
            "\n",
            "Epoch 10: Precision=0.9375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9843    0.9876    0.9859      1206\n",
            "         1.0     0.9180    0.8984    0.9081       187\n",
            "\n",
            "    accuracy                         0.9756      1393\n",
            "   macro avg     0.9512    0.9430    0.9470      1393\n",
            "weighted avg     0.9754    0.9756    0.9755      1393\n",
            "\n",
            "Epoch 11: Precision=0.9180\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9819    0.9917    0.9868      1206\n",
            "         1.0     0.9429    0.8824    0.9116       187\n",
            "\n",
            "    accuracy                         0.9770      1393\n",
            "   macro avg     0.9624    0.9370    0.9492      1393\n",
            "weighted avg     0.9767    0.9770    0.9767      1393\n",
            "\n",
            "Epoch 12: Precision=0.9429\n",
            "Early stopping due to no improvement in precision.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('all')\n",
        "\n",
        "# -------------------\n",
        "# 1. Load Dataset\n",
        "# -------------------\n",
        "df = pd.read_csv(\"sms.csv\")\n",
        "\n",
        "# Encode labels: ham = 0, spam = 1\n",
        "# df['target'] = LabelEncoder().fit_transform(df['target'])\n",
        "df['target'] = df['target'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english') and word not in string.punctuation]\n",
        "\n",
        "    # Apply stemming\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(preprocess)\n",
        "\n",
        "# -------------------\n",
        "# 3. Vectorize Text\n",
        "# -------------------\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
        "y = df['target'].values\n",
        "\n",
        "# -------------------\n",
        "# 4. Train-Test Split\n",
        "# -------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# -------------------\n",
        "# 5. Dataset & Dataloader\n",
        "# -------------------\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(SpamDataset(X_train_tensor, y_train_tensor), batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(SpamDataset(X_test_tensor, y_test_tensor), batch_size=16)\n",
        "\n",
        "# -------------------\n",
        "# 6. Model Definition\n",
        "# -------------------\n",
        "class SpamANN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SpamANN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = SpamANN(X_train.shape[1])\n",
        "\n",
        "# -------------------\n",
        "# 7. Training Setup\n",
        "# -------------------\n",
        "# pos_weight handles class imbalance: spam class is underrepresented\n",
        "pos_weight = torch.tensor([(y_train == 0).sum() / (y_train == 1).sum()], dtype=torch.float32)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# -------------------\n",
        "# 8. Evaluation Function\n",
        "# -------------------\n",
        "from sklearn.metrics import classification_report, precision_score\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            output = model(X_batch).squeeze(1)\n",
        "            pred = torch.round(torch.sigmoid(output))\n",
        "            preds.extend(pred.tolist())\n",
        "            labels.extend(y_batch.tolist())\n",
        "    print(classification_report(labels, preds, digits=4))\n",
        "\n",
        "    # Return precision so training loop can compare and save best\n",
        "    return precision_score(labels, preds, zero_division=0)\n",
        "\n",
        "# -------------------\n",
        "# 9. Training Loop\n",
        "# -------------------\n",
        "best_precision = 0\n",
        "patience = 5\n",
        "no_improve_epochs = 0\n",
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch).squeeze(1)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    precision = evaluate(model, test_loader)\n",
        "\n",
        "    if precision > best_precision:\n",
        "        best_precision = precision\n",
        "        torch.save(model.state_dict(), \"torch.pth\")\n",
        "        no_improve_epochs = 0\n",
        "        improvement = \" <-- Best Precision!\"\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        improvement = \"\"\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Precision={precision:.4f}{improvement}\")\n",
        "\n",
        "    if no_improve_epochs >= patience:\n",
        "        print(\"Early stopping due to no improvement in precision.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(['hi','hello','nepal'])"
      ],
      "metadata": {
        "id": "B_S3jeUbJhqJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f2f80a9-7f4c-47d4-8b90-be4b8d463583"
      },
      "id": "B_S3jeUbJhqJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi hello nepal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa9b80dd",
      "metadata": {
        "id": "aa9b80dd"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save vectorizer\n",
        "with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25c0200",
      "metadata": {
        "id": "f25c0200"
      },
      "outputs": [],
      "source": [
        "import torch, pickle, re, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remove non-alphanumeric characters\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english') and word not in string.punctuation]\n",
        "\n",
        "    # Apply stemming\n",
        "    tokens = [ps.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "with open(\"tfidf_vectorizer.pkl\", \"rb\") as f:\n",
        "    vectorizer = pickle.load(f)\n",
        "\n",
        "class SpamANN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__();\n",
        "      self.model=torch.nn.Sequential(torch.nn.Linear(5000,128),torch.nn.ReLU(),torch.nn.Dropout(0.3),torch.nn.Linear(128,64),torch.nn.ReLU(),torch.nn.Linear(64,1))\n",
        "    def forward(self,x): return self.model(x)\n",
        "\n",
        "model = SpamANN()\n",
        "model.load_state_dict(torch.load(\"torch.pth\"))\n",
        "model.eval()\n",
        "\n",
        "def predict(text):\n",
        "    x = vectorizer.transform([preprocess(text)]).toarray()\n",
        "    with torch.no_grad():\n",
        "        p = torch.sigmoid(model(torch.tensor(x, dtype=torch.float32))).item()\n",
        "    return \"SPAM\" if p>=0.5 else \"HAM\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a8667f1",
      "metadata": {
        "id": "1a8667f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5dc203c1-a218-4510-ea28-275ca2b4d270"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SPAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "predict(\"Congratulations! You've won a $1000 Walmart gift card. Click here to claim your prize.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02559779",
      "metadata": {
        "id": "02559779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7800cc87-fb25-4c93-9943-d457338dc9b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SPAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "predict(\"Win a trip to Paris! Text WIN to 12345 to enter the contest.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147a5bcb",
      "metadata": {
        "id": "147a5bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3d0bc3eb-a998-4115-a9bd-e34c99f30c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "predict(\"Hey, how are you doing? Let's catch up soon!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2a6799",
      "metadata": {
        "id": "be2a6799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4268a876-24a4-4c6a-8568-dc394bd6d5f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "predict(\"Hey, are we still meeting for coffee tomorrow at 3 PM?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08526971",
      "metadata": {
        "id": "08526971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c132cea-8455-4af9-cdac-1f8cc3bebc7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "predict(\"The restaurant reservation for 7 PM is confirmed. Looking forward to seeing you.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbb1fcc",
      "metadata": {
        "id": "5fbb1fcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4ab423-c5cf-45e8-9942-c5a64c9e3929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPAM\n"
          ]
        }
      ],
      "source": [
        "print(predict(\"Free entry in 2 a weekly competition!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce6a9af",
      "metadata": {
        "id": "0ce6a9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fd847ea1-4ba4-43d8-ddf4-ce97c0e48498"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "predict(\"The restaurant reservation for 7 PM is confirmed. Looking forward to seeing you.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "846e211d",
      "metadata": {
        "id": "846e211d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nabin",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}